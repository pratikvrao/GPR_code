{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "from scipy import io\n",
    "from scipy.stats import entropy, kde, multivariate_normal, wasserstein_distance\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mutual_info_score, log_loss\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import sklearn.gaussian_process as gp\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "# from athena.active import ActiveSubspaces\n",
    "import math\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.spatial import KDTree\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594636de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.GPR_build import GPR_fit\n",
    "from core.preprocess import scale_data, inverse_transform\n",
    "from core.estimate_pdf import estimate_pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the data\n",
    "\"\"\"\n",
    "## load CCF data of Jan and July at trg (all levels)\n",
    "iRF_global = io.loadmat('data_for_training/iRF_global_trg.mat')\n",
    "sorted(iRF_global.keys())\n",
    "\n",
    "## load interpolated weather data of Jan and July at trg (all levels)\n",
    "weather_global = io.loadmat('data_for_training/weather_global_trg.mat') #using dt=1, which is correct\n",
    "sorted(weather_global.keys())\n",
    "\n",
    "\n",
    "## Extract CCF values (observations)\n",
    "iRF = iRF_global['iRF'].flatten('F')\n",
    "\n",
    "# print(iRF)\n",
    "\n",
    "## Extract meteorological features\n",
    "temp = weather_global['tm1'].flatten('F')\n",
    "geopot = weather_global['geopot'].flatten('F')\n",
    "zi0 = weather_global['zi0'].flatten('F') #careful, redundancy\n",
    "u = weather_global['u'].flatten('F')\n",
    "\n",
    "## Extact latitudinal data (Release)\n",
    "lon = np.linspace(-115,150,num=len(iRF),endpoint=True) \n",
    "lat = np.linspace(-50,85,num=len(iRF),endpoint=True)\n",
    "\n",
    "\n",
    "## Create pandas dataframe\n",
    "\n",
    "data = {\"iRF\":iRF, \"temp\":temp, \"geopot\":geopot,  \\\n",
    "        \"zi0\":zi0,  \\\n",
    "        \"u\":u,  \"lat\":lat}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "data_LR = {\"iRF\":iRF, \"temp\":temp, \"geopot\":geopot,  \\\n",
    "          \"Tphi\":temp*geopot}\n",
    "\n",
    "data_LR = pd.DataFrame(data_LR)\n",
    "\n",
    "\n",
    "\n",
    "X = data.drop([\"iRF\"], 1) #choose all features for GPR\n",
    "X2 = data_LR.drop([\"iRF\"], 1) #choose original features for LR\n",
    "y = data[[\"iRF\"]]\n",
    "\n",
    "\"\"\"\n",
    "Check contents and shape of data\n",
    "\"\"\"\n",
    "print(X.shape,y.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect outlier(s) using Tukey's method\n",
    "\"\"\"\n",
    "\n",
    "# The data to be analysed\n",
    "yd = y\n",
    "\n",
    "\n",
    "q1 = np.quantile(yd, 0.25) #1st quartile\n",
    "q3 = np.quantile(yd, 0.75) #3rd quartile\n",
    "med = np.median(yd)\n",
    " \n",
    "\n",
    "iqr = q3-q1 #Inter quartile Range (IQR)\n",
    " \n",
    "lb = q1 - (1.5*iqr) #lower bound\n",
    "ub = q3 + (1.5*iqr) #upper bound\n",
    "\n",
    "# print(lb, ub, med, np.mean(np.asarray(yd)))\n",
    "\n",
    "# finding outliers\n",
    "outliers = np.asarray(yd)[(np.asarray(yd) <= lb) | (np.asarray(yd) >= ub)]\n",
    "outliers_round = np.round(outliers,8)\n",
    "print(f'The following are the outliers in the boxplot:{outliers_round}')\n",
    "\n",
    "idx_outliers = np.where(np.isin(y, outliers.flatten()))\n",
    "idx_outliers = np.squeeze(idx_outliers)[0]\n",
    "print(f'The corresponding indices are:{idx_outliers}')\n",
    "\n",
    "#Boxplot\n",
    "_ = plt.figure(figsize=(6,4))\n",
    "plt.boxplot(yd)\n",
    "plt.xlabel('')\n",
    "plt.ylabel(r'iRF [W/m$^2$]',fontsize=14)\n",
    "plt.tick_params(axis='y', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92992b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Outliers must only be removed if there is an expected error\n",
    "\"\"\"\n",
    "\n",
    "outlier_removal = False \n",
    "\n",
    "if outlier_removal:\n",
    "    y = np.delete(y.values, idx_outliers)\n",
    "    X = np.delete(X.values, idx_outliers, axis=0)\n",
    "    X2 = np.delete(X2.values, idx_outliers, axis=0)\n",
    "    print(y.shape,X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the data into train (80%) and test (20%) groups\n",
    "\"\"\"\n",
    "\n",
    "#Randomly split the data  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scale the data matrix (X,y) are of the same order of magnitude\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if outlier_removal:\n",
    "    for var in [y_train, y_train2, y_test, y_test2]:\n",
    "        var.shape = (-1, 1)  # Reshape in-place\n",
    "\n",
    "\n",
    "# Scale both datasets\n",
    "X_train, X_test, y_train, y_test, scalerX, scalery = scale_data(X_train, X_test, y_train, y_test)\n",
    "X_train2, X_test2, y_train2, y_test2, scalerX2, scalery2 = scale_data(X_train2, X_test2, y_train2, y_test2)\n",
    "\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit the standard GPR model to the data\n",
    "\"\"\"\n",
    "\n",
    "y_pred_test, y_std, y_samples, model = GPR_fit(X_train, y_train, X_test)\n",
    "\n",
    "print(model.kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_test.shape, y_samples.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d80f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualise the Kernel Density Estimate (KDE) of the test data and predictions from the standard GPR model\n",
    "\"\"\"\n",
    "## Build KDE for all samples generated by GPR model since it contains variance information, \n",
    "##  not just the mean\n",
    "p_y_data, p_y_gp, test = estimate_pdf(y1=y_test,y2=y_samples,pts=2000,plot_fig=True,label1='observations',label2='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Put back the data on the original scale\n",
    "\"\"\"\n",
    "\n",
    "# Inverse transform first set\n",
    "y_train_inverse, y_pred_inverse, y_test_inverse = inverse_transform(scalery, y_train, y_pred_test, y_test)\n",
    "X_train_inverse, X_test_inverse = inverse_transform(scalerX, X_train, X_test)\n",
    "y_std_inverse, y_samples_inverse = inverse_transform(scalery, y_std, y_samples)\n",
    "\n",
    "print(y_samples_inverse.shape)\n",
    "\n",
    "# Inverse transform second set\n",
    "y_train_inverse2, y_test_inverse2 = inverse_transform(scalery2, y_train2, y_test2)\n",
    "X_train_inverse2, X_test_inverse2 = inverse_transform(scalerX2, X_train2, X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparing the data and predictions from the standard GPR model\n",
    "- R2\n",
    "- KL-divergence\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Scatter plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "plt.scatter(y_test_inverse, y_pred_inverse, color='red', s = 20)\n",
    "plt.plot(y_test_inverse, y_test_inverse, 'k')\n",
    "plt.xlabel('CCM iRF [W/m$^2$]')\n",
    "plt.ylabel('Predicted iRF [W/m$^2$]')\n",
    "plt.title('Standard GP')\n",
    "plt.axis('square')\n",
    "# plt.savefig('GPR_fit.png', dpi=300, bbox_inches='tight')\n",
    "print(\"R\\u00b2 score of GPR is = \",r2_score(y_test_inverse, y_pred_inverse))\n",
    "print(\"KL-div(pred,obs) is = \",entropy(p_y_gp, p_y_data)) #asymmetric 'metric'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4819749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Apply Linear Regression - Manen and Grewe's features\"\n",
    "\n",
    "sk_model = LinearRegression()\n",
    "sk_model.fit(X_train2, y_train2)\n",
    "beta0 = sk_model.intercept_\n",
    "beta = sk_model.coef_\n",
    "# y_pred_test_LR = sk_model.predict(X_train2)\n",
    "y_pred_test_LR = sk_model.predict(X_test2)\n",
    "y_pred_LR_inverse = scalery2.inverse_transform(y_pred_test_LR.reshape(-1,1))\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19664ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(y_test_inverse, y_test_inverse2) #this should be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93292b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"Apply Linear Regression - Manen and Grewe's features\"\n",
    "\n",
    "maroon = (212/255, 17/255, 89/255)\n",
    "blue = (26/255, 133/255, 255/255) \n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(y_test_inverse, y_pred_inverse, color = maroon, s = 20, label = 'Standard GP, R$^2$ = 0.53')\n",
    "plt.scatter(y_test_inverse2, y_pred_LR_inverse, color = blue, s = 20, label = 'Linear Regression, R$^2$ = 0.05')\n",
    "plt.plot(y_test_inverse, y_test_inverse, 'k')\n",
    "plt.xlabel('CCM iRF [W m$^{-2}$]')\n",
    "plt.ylabel('Predicted iRF [W m$^{-2}$]')\n",
    "# plt.title(\"R\\u00b2 score of GPR is = \", round(r2_score(y_test_inverse, y_pred_inverse),2))\n",
    "plt.legend(fontsize=15)\n",
    "plt.axis('square')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('GPR_LR_MG_features.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"R\\u00b2 score of GPR is = \", round(r2_score(y_test_inverse, y_pred_inverse),2))\n",
    "print(\"R\\u00b2 score of LR is = \", round(r2_score(y_test_inverse, y_pred_LR_inverse),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b565914",
   "metadata": {},
   "source": [
    "# Train and test the heteroscedastic GPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d48f9",
   "metadata": {},
   "source": [
    "#### Some common packages are imported again if you want to make a separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe31945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import gpflow as gpf\n",
    "\n",
    "from gpflow.ci_utils import reduce_in_tests\n",
    "from gpflow.utilities import print_summary\n",
    "gpf.config.set_default_summary_fmt(\"notebook\")\n",
    "optimizer_config = dict(maxiter=reduce_in_tests(1000))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.GPR_build import train_chainedGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood, kernel, inducing_variable = build_chainedGP(X_train, n_latents=2, M_offset=10, kernel_variance=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eccf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Declare likelihood, latent functions for mean and variance for chained GPR model\"\n",
    "\n",
    "############################################################## likelihood\n",
    "\n",
    "likelihood = gpf.likelihoods.HeteroskedasticTFPConditional(\n",
    "    distribution_class=tfp.distributions.Normal,  # Gaussian Likelihood\n",
    "#     distribution_class=tfp.distributions.LogNormal(),  # \n",
    "    scale_transform=tfp.bijectors.Softplus(),  # (Softplus Transform ()originally: Exp Transform. option: Softplus)\n",
    ")\n",
    "############################################################## kernel\n",
    "kernel = gpf.kernels.SeparateIndependent(\n",
    "    [\n",
    "        gpf.kernels.SquaredExponential(lengthscales=np.ones(X_train.shape[1]),variance=1.0),  # This is k1, the kernel of f1\n",
    "        gpf.kernels.SquaredExponential(variance=1.0),    # this is k2, the kernel of f2\n",
    "    ]\n",
    ")\n",
    "############################################################## inducing_variable\n",
    "M = len(X_train)-10 # this can be any value less than len(X_train). Only these many points are used to parametrize the covariance matrix.\n",
    "Z = KMeans(n_clusters=M).fit(X_train).cluster_centers_\n",
    "\n",
    "inducing_variable = gpf.inducing_variables.SeparateIndependentInducingVariables(\n",
    "    [\n",
    "        gpf.inducing_variables.InducingPoints(Z),  # This is Uf = f(Z)\n",
    "        gpf.inducing_variables.InducingPoints(Z),  # This is Ug = g(Z)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Likelihood's expected latent_dim: {likelihood.latent_dim}\") #Number of latent functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = map(pd.DataFrame, [X_train, X_test, y_train, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3313309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train the chained GPR model with multiple epochs\"\n",
    "\n",
    "epochs = 800\n",
    "alpha = 0.01\n",
    "#n = 0\n",
    "plt.figure()\n",
    "model, loss = train_chainedGP(X_train, y_train, kernel, likelihood, inducing_variable, epochs, alpha)\n",
    "sns.lineplot(np.arange(0, epochs), loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Mean and variance of model predictions over the test set\"\n",
    "\n",
    "mean_y, var_y = model.predict_y(X_test.values)\n",
    "sigma_y = np.sqrt(var_y)\n",
    "\n",
    "n_samples = 5000\n",
    "chained_samples_y = []\n",
    "for i in range(len(X_test.values)):\n",
    "    chained_samples_y.append(np.random.normal(mean_y[i], sigma_y[i], n_samples))\n",
    "    \n",
    "chained_samples_flat = [item for sublist in chained_samples_y for item in sublist] # flatten the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_y.shape,var_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(chained_samples_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7261952",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chained_samples_y), len(chained_samples_flat))\n",
    "\n",
    "chained_samples_y = np.asarray(chained_samples_y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ca281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(chained_samples_y), np.shape(chained_samples_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_chained = []\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    y_pred_chained = np.hstack((y_pred_chained, chained_samples_y[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16387288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(y_pred_chained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c784116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Placing the predictions on the real scale\"\n",
    "\n",
    "y_pred_chained_inverse = scalery.inverse_transform(y_pred_chained.reshape(-1,1))\n",
    "y_pred_chained_mean = np.mean(y_pred_chained.reshape(-1, n_samples), axis=1)\n",
    "y_pred_chained_mean_inverse = np.mean(y_pred_chained_inverse.reshape(-1, n_samples), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0653c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Build KDE for all samples generated by both GP models since they contain variance information,not just the mean\"\n",
    "\n",
    "p_y_standard, p_y_chained, test = estimate_pdf(y1=y_samples, y2=y_pred_chained, pts=2000, plot_fig=False, label1='standard GPR', label2='chained GPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_pred_chained_inverse.reshape(-1,1)),np.std(y_pred_chained_inverse.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12074c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Evaluate performance based on certain aspects\"\n",
    "\n",
    "print(\"WD1(pred,obs) for standard model is = \", np.round(wasserstein_distance(y_test.values.reshape(-1), y_samples.reshape(-1)), 3)) #symmetric metric\n",
    "print(\"KL-div(pred,obs) for standard model is = \",np.round(entropy(p_y_standard, p_y_data), 3)) #not symmetric!\n",
    "print(\"R\\u00b2 score of standard model is = \",np.round(r2_score(y_test, y_pred_test),2))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.tick_params(left = True, right = False , labelleft = True ,\n",
    "                labelbottom = False, bottom = False)\n",
    "plt.plot(test,p_y_data,color=\"blue\",label=\"CCM data\")\n",
    "plt.plot(test,p_y_standard,color=\"red\",label=\"Standard GP\")\n",
    "plt.plot(test,p_y_chained,color=\"green\",label=\"Chained GP\")\n",
    "\n",
    "plt.xlabel('$y{_*}$')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(fontsize=15)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n",
    "# plt.savefig(f'model_compare.png', dpi=300, bbox_inches='tight') \n",
    "\n",
    "\n",
    "print(\"WD1(pred,obs) for chained model is = \", np.round(wasserstein_distance(y_test.values.reshape(-1), y_pred_chained), 3)) #symmetric \n",
    "print(\"KL-div(pred,obs) for chained model is = \",np.round(entropy(p_y_chained, p_y_data), 3)) #not!\n",
    "print(\"R\\u00b2 score of chained model is = \", np.round(r2_score(y_test, y_pred_chained_mean),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa88377",
   "metadata": {},
   "source": [
    "#### Figure 3a: Chained GPR mean predictions against test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test_inverse, y_pred_chained_mean_inverse, color = \"teal\", s = 20, label = 'Chained GP')\n",
    "plt.plot(y_test_inverse, y_test_inverse, 'k')\n",
    "plt.xlabel('CCM iRF [W m$^{-2}$]')\n",
    "plt.ylabel('Predicted iRF [W m$^{-2}$]')\n",
    "plt.title('Chained GP')\n",
    "plt.axis('square')\n",
    "plt.savefig('GPR_fit_chained.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9e41c",
   "metadata": {},
   "source": [
    "#### Violin plots in Figure 3b and supplementary material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The following are the outliers in the boxplot:{outliers_round}')\n",
    "\n",
    "test_outliers = np.where(np.isin(y_test_inverse, outliers.flatten()))\n",
    "test_outliers = np.squeeze(test_outliers)[0]\n",
    "print(f'The corresponding indices are:{test_outliers}')\n",
    "\n",
    "random.seed(4) #used for Figure 3(b)\n",
    "z = random.sample(list(range(0, len(X_test))), 20)\n",
    "print(f'The randomly chosen test indices are = {z}') #one of the outliers get randomly included in this set\n",
    "\n",
    "z[-5:] = test_outliers[-5:] #replcaing the last five entries with remianing outliers for later visualisation\n",
    "\n",
    "\n",
    "z[3], z[-6] = z[-6], z[3] #swapping positions to include all outliers at the end\n",
    "\n",
    "print(f'The randomly chosen test indices including all test outliers are = {z}')\n",
    "print(f'Sanity check: {z[-6:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c612857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature indices and labels\n",
    "fidx = range(X_test.shape[1])\n",
    "flist = [\n",
    "    \"Temperature [K]\",\n",
    "    \"Geopotential [m$^2$ s$^{-2}$]\",\n",
    "    r\"Solar irradiance [W m$^{-2}$]\",\n",
    "    \"Wind velocity [m s$^{-1}$]\",\n",
    "    r\"Release location [$^{\\circ}$]\"\n",
    "]\n",
    "\n",
    "# Reshape prediction data\n",
    "data = y_pred_chained_inverse.reshape(len(X_test), n_samples)[z].T  # Chained GPR\n",
    "data2 = y_samples_inverse.reshape(len(X_test), n_samples)[z].T      # Standard GPR\n",
    "\n",
    "# Variance for color mapping\n",
    "variances = np.std(data, axis=0)\n",
    "variances2 = np.std(data2, axis=0)\n",
    "\n",
    "# Color maps\n",
    "cmap = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(variances), vmax=max(variances)))\n",
    "\n",
    "# Violin widths based on feature scale\n",
    "widths = [np.mean(np.abs(np.diff(X_test_inverse[:, i]))) / 2 for i in range(X_test.shape[1])]\n",
    "\n",
    "# Plot per feature\n",
    "for j in fidx:\n",
    "    fig, axs = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "    # ======= STANDARD GPR =======\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.set_title('Standard GP')\n",
    "\n",
    "    ax1.scatter(X_test_inverse[z, j], y_test_inverse[z], s=12, color=\"orange\", label=\"CCM data\")\n",
    "    ax1.scatter(X_test_inverse[z[-6:], j], y_test_inverse[z[-6:]], marker='s', s=100, facecolors='none', edgecolors='magenta')\n",
    "    ax1.scatter(X_test_inverse[z, j], y_pred_inverse[z], s=12, color=\"blue\", label=\"mean prediction\")\n",
    "\n",
    "    violins_std = ax1.violinplot(data2, X_test_inverse[z, j], widths=widths[j], showextrema=False, points=n_samples, bw_method='scott')\n",
    "\n",
    "    for i, pc in enumerate(violins_std['bodies']):\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_facecolor(cmap.to_rgba(variances2[i]))\n",
    "\n",
    "    cbar_std = plt.colorbar(cmap, ax=ax1)\n",
    "    cbar_std.set_label('Variance [W$^2$ m$^{-4}$]')\n",
    "\n",
    "    ax1.set_xlabel(flist[j])\n",
    "    ax1.set_ylabel(\"iRF [W m$^{-2}$]\")\n",
    "    ax1.set_ylim(top=1e-4)\n",
    "    ax1.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n",
    "    ax1.legend(loc=\"upper center\", fontsize=14)\n",
    "\n",
    "    # ======= CHAINED GPR =======\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.set_title('Chained GP')\n",
    "\n",
    "    ax2.scatter(X_test_inverse[z, j], y_test_inverse[z], s=12, color=\"orange\", label=\"CCM data\")\n",
    "    ax2.scatter(X_test_inverse[z[-6:], j], y_test_inverse[z[-6:]], marker='s', s=100, facecolors='none', edgecolors='magenta')\n",
    "    ax2.scatter(X_test_inverse[z, j], np.mean(y_pred_chained_inverse.reshape(-1, n_samples), axis=1)[z], s=12, color=\"black\", label=\"mean prediction\")\n",
    "\n",
    "    violins_chain = ax2.violinplot(data, X_test_inverse[z, j], widths=widths[j], showextrema=False, points=n_samples, bw_method='scott')\n",
    "\n",
    "    for i, pc in enumerate(violins_chain['bodies']):\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_facecolor(cmap.to_rgba(variances[i]))\n",
    "\n",
    "    cbar_chain = plt.colorbar(cmap, ax=ax2)\n",
    "    cbar_chain.set_label('Variance [W$^2$ m$^{-4}$]')\n",
    "\n",
    "    ax2.set_xlabel(flist[j])\n",
    "    ax2.set_ylabel(\"iRF [W m$^{-2}$]\")\n",
    "    ax2.set_ylim(top=1e-4)\n",
    "    ax2.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n",
    "    ax2.legend(loc=\"best\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    # plt.savefig(f'violinplot_{j}.png', dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d32be4",
   "metadata": {},
   "source": [
    "\n",
    "### Climate impact prediction of arbitrary flight paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.load_trajectory_data import load_trajectory_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec38cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Load trajectory data from EUROCONTROL and AirTraf\"\n",
    "\n",
    "trajectory_info = [\n",
    "    ('traj1.mat', '1', 'flight_latitude_f1'),\n",
    "    ('traj2.mat', '2', 'flight_latitude_f2'),\n",
    "    ('traj3.mat', '3', 'flight_latitude_f3'),\n",
    "    ('traj4_AirTraf.mat', '4', 'flight_latitude_f4')\n",
    "]\n",
    "\n",
    "\n",
    "X_traj1, X_traj2, X_traj3, X_traj4 = [\n",
    "    load_trajectory_data(os.path.join(\"trajectory_data\", fname), suffix, lat_key)\n",
    "    for fname, suffix, lat_key in trajectory_info\n",
    "]\n",
    "\n",
    "print(X_traj1.shape, X_traj2.shape, X_traj3.shape, X_traj4.shape) #AirTraf trajectory will have 101 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traj4.t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Remove take off and landing part for AirTraf\"\n",
    "\n",
    "t_4 = X_traj4.t_4[12:-11]\n",
    "g_4 = X_traj4.g_4[12:-11]\n",
    "zi0_4 = X_traj4.zi0_4[12:-11]\n",
    "u_4 = X_traj4.u_4[12:-11]\n",
    "Rl_4 = X_traj4.Rl_4[12:-11]\n",
    "\n",
    "print(Rl_4)\n",
    "\n",
    "#interpolate to size of other flights\n",
    "\n",
    "t_4 = np.interp(np.linspace(0, 1, X_traj1.shape[0]), np.linspace(0, 1, len(t_4)), t_4)\n",
    "g_4 = np.interp(np.linspace(0, 1, X_traj1.shape[0]), np.linspace(0, 1, len(g_4)), g_4)\n",
    "zi0_4 = np.interp(np.linspace(0, 1, X_traj1.shape[0]), np.linspace(0, 1, len(zi0_4)), zi0_4)\n",
    "u_4 = np.interp(np.linspace(0, 1, X_traj1.shape[0]), np.linspace(0, 1, len(u_4)), u_4)\n",
    "Rl_4 = np.interp(np.linspace(0, 1, X_traj1.shape[0]), np.linspace(0, 1, len(Rl_4)), Rl_4)\n",
    "\n",
    "print(Rl_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a495699",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traj4 = {\"t_4\":t_4, \"g_4\":g_4,  \\\n",
    "        \"zi0_4\":zi0_4,  \\\n",
    "        \"u_4\":u_4,  \"Rl_4\":Rl_4}\n",
    "\n",
    "X_traj4 = pd.DataFrame(X_traj4)\n",
    "\n",
    "print(X_traj1.shape, X_traj2.shape, X_traj3.shape, X_traj4.shape) #they are all of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Applying the same logic as before and predicting the climate impact of the trajectory data\"\n",
    "\n",
    "scalerXtraj1 = StandardScaler().fit(X_traj1)\n",
    "X_traj1 = scalerXtraj1.transform(X_traj1)\n",
    "scalerXtraj2 = StandardScaler().fit(X_traj2)\n",
    "X_traj2 = scalerXtraj2.transform(X_traj2)\n",
    "scalerXtraj3 = StandardScaler().fit(X_traj3)\n",
    "X_traj3 = scalerXtraj3.transform(X_traj3)\n",
    "scalerXtraj4 = StandardScaler().fit(X_traj4)\n",
    "X_traj4 = scalerXtraj4.transform(X_traj4)\n",
    "\n",
    "mean_ytraj1, var_ytraj1 = model.predict_y(X_traj1)\n",
    "sigma_ytraj1 = np.sqrt(var_ytraj1)\n",
    "\n",
    "mean_ytraj2, var_ytraj2 = model.predict_y(X_traj2)\n",
    "sigma_ytraj2 = np.sqrt(var_ytraj2)\n",
    "\n",
    "mean_ytraj3, var_ytraj3 = model.predict_y(X_traj3)\n",
    "sigma_ytraj3 = np.sqrt(var_ytraj3)\n",
    "\n",
    "mean_ytraj4, var_ytraj4 = model.predict_y(X_traj4)\n",
    "sigma_ytraj4 = np.sqrt(var_ytraj4)\n",
    "\n",
    "\n",
    "chained_samples_ytraj1 = []\n",
    "chained_samples_ytraj2 = []\n",
    "chained_samples_ytraj3 = []\n",
    "chained_samples_ytraj4 = []\n",
    "\n",
    "#length of feature vector is not the same for all trajectories!\n",
    "for i in range(len(X_traj1)):\n",
    "    chained_samples_ytraj1.append(np.random.normal(mean_ytraj1[i], sigma_ytraj1[i], n_samples))\n",
    "    \n",
    "for i in range(len(X_traj2)):\n",
    "    chained_samples_ytraj2.append(np.random.normal(mean_ytraj2[i], sigma_ytraj2[i], n_samples))\n",
    "    \n",
    "for i in range(len(X_traj3)):\n",
    "    chained_samples_ytraj3.append(np.random.normal(mean_ytraj3[i], sigma_ytraj3[i], n_samples)) \n",
    "    \n",
    "for i in range(len(X_traj4)):\n",
    "    chained_samples_ytraj4.append(np.random.normal(mean_ytraj4[i], sigma_ytraj4[i], n_samples))     \n",
    "    \n",
    "    \n",
    "chained_samples_ytraj1 = np.asarray(chained_samples_ytraj1).T\n",
    "chained_samples_ytraj2 = np.asarray(chained_samples_ytraj2).T\n",
    "chained_samples_ytraj3 = np.asarray(chained_samples_ytraj3).T\n",
    "chained_samples_ytraj4 = np.asarray(chained_samples_ytraj4).T\n",
    "\n",
    "y_pred_traj1 = []\n",
    "y_pred_traj2 = []\n",
    "y_pred_traj3 = []\n",
    "y_pred_traj4 = []\n",
    "\n",
    "#length of feature vector is not the same for all trajectories!\n",
    "for i in range(0, len(X_traj1)):\n",
    "    y_pred_traj1 = np.hstack((y_pred_traj1, chained_samples_ytraj1[:,i]))\n",
    "    \n",
    "for i in range(0, len(X_traj2)):\n",
    "    y_pred_traj2 = np.hstack((y_pred_traj2, chained_samples_ytraj2[:,i]))\n",
    "\n",
    "for i in range(0, len(X_traj3)):\n",
    "    y_pred_traj3 = np.hstack((y_pred_traj3, chained_samples_ytraj3[:,i]))   \n",
    "\n",
    "for i in range(0, len(X_traj4)):\n",
    "    y_pred_traj4 = np.hstack((y_pred_traj4, chained_samples_ytraj4[:,i]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traj1_inverse = scalerXtraj1.inverse_transform(X_traj1)\n",
    "X_traj2_inverse = scalerXtraj2.inverse_transform(X_traj2)\n",
    "X_traj3_inverse = scalerXtraj3.inverse_transform(X_traj3)\n",
    "X_traj4_inverse = scalerXtraj4.inverse_transform(X_traj4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The impact is based on the fact that 5e5 kg(NO) is released per trg in the CCM simulation. This should be converted to kg(NO2)\n",
    "#by using the molecular weight ratio of NO2 to NO, which makes it 7.665e5 kg(NO2). Thus our prediction is W/m2/kg(NO2).\n",
    "\n",
    "scale_factor = 7.665e5\n",
    "y_pred_traj1_inverse = scalery.inverse_transform(y_pred_traj1.reshape(-1,1))/scale_factor\n",
    "y_pred_traj2_inverse = scalery.inverse_transform(y_pred_traj2.reshape(-1,1))/scale_factor\n",
    "y_pred_traj3_inverse = scalery.inverse_transform(y_pred_traj3.reshape(-1,1))/scale_factor\n",
    "y_pred_traj4_inverse = scalery.inverse_transform(y_pred_traj4.reshape(-1,1))/scale_factor\n",
    "y_mean_traj1_inverse = scalery.inverse_transform(mean_ytraj1.reshape(-1,1))/scale_factor\n",
    "y_mean_traj2_inverse = scalery.inverse_transform(mean_ytraj2.reshape(-1,1))/scale_factor\n",
    "y_mean_traj3_inverse = scalery.inverse_transform(mean_ytraj3.reshape(-1,1))/scale_factor\n",
    "y_mean_traj4_inverse = scalery.inverse_transform(mean_ytraj4.reshape(-1,1))/scale_factor\n",
    "\n",
    "\n",
    "y_sigma_traj1_inverse = sigma_ytraj1.reshape(-1,1)*scalery.scale_/scale_factor\n",
    "y_sigma_traj2_inverse = sigma_ytraj2.reshape(-1,1)*scalery.scale_/scale_factor\n",
    "y_sigma_traj3_inverse = sigma_ytraj3.reshape(-1,1)*scalery.scale_/scale_factor\n",
    "y_sigma_traj4_inverse = sigma_ytraj4.reshape(-1,1)*scalery.scale_/scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93bf0b",
   "metadata": {},
   "source": [
    "#### Figure 5 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Visualising the impact of the trajectories\"\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,6))\n",
    "plt.plot(X_traj4_inverse[:,-1],np.mean(y_pred_traj2_inverse.reshape(-1,n_samples),axis=1), color=\"#000000\", marker='s', label=\"Flight 1\")\n",
    "plt.plot(X_traj4_inverse[:,-1],np.mean(y_pred_traj1_inverse.reshape(-1,n_samples),axis=1), color=\"#1E88E5\", marker='o', label=\"Flight 2\")\n",
    "plt.plot(X_traj4_inverse[:,-1],np.mean(y_pred_traj3_inverse.reshape(-1,n_samples),axis=1), color=\"#D81B60\", marker='^',label=\"Flight 3\")\n",
    "plt.plot(X_traj4_inverse[:,-1],np.mean(y_pred_traj4_inverse.reshape(-1,n_samples),axis=1), color=\"#FFC107\", linestyle='dashed', label=\"Great circle\")\n",
    "plt.ylabel(\"iRF [W m$^{-2}$ kg(NO$_2$)$^{-1}$]\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "# plt.xlabel(\"Cruise flight completion (%)\")\n",
    "plt.fill_between(\n",
    "    X_traj4_inverse[:,-1],\n",
    "#     np.linspace(0,100,18).round()\n",
    "    (y_mean_traj1_inverse.ravel() - y_sigma_traj1_inverse.ravel()),\n",
    "    (y_mean_traj1_inverse.ravel() + y_sigma_traj1_inverse.ravel()),\n",
    "    color=\"#1E88E5\", alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    X_traj4_inverse[:,-1],\n",
    "#     np.linspace(0,100,18).round()\n",
    "    (y_mean_traj2_inverse.ravel() - y_sigma_traj2_inverse.ravel()),\n",
    "    (y_mean_traj2_inverse.ravel() + y_sigma_traj2_inverse.ravel()),\n",
    "    color=\"#000000\", alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    X_traj4_inverse[:,-1],\n",
    "#     np.linspace(0,100,18).round()\n",
    "    (y_mean_traj3_inverse.ravel() - y_sigma_traj3_inverse.ravel()),\n",
    "    (y_mean_traj3_inverse.ravel() + y_sigma_traj3_inverse.ravel()),\n",
    "    color=\"#D81B60\", alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    X_traj4_inverse[:,-1],\n",
    "#     np.linspace(0,100,18).round()\n",
    "    (y_mean_traj4_inverse.ravel() - y_sigma_traj4_inverse.ravel()),\n",
    "    (y_mean_traj4_inverse.ravel() + y_sigma_traj4_inverse.ravel()),\n",
    "    color=\"#FFC107\", alpha=0.2,\n",
    ")\n",
    "# axes.invert_xaxis()\n",
    "# plt.xticks(np.arange(0, 18, 1))\n",
    "xtick_positions = [51, 45, 42]  # Specify the positions where you want ticks\n",
    "xtick_labels = [r'50$^{\\circ}$N', '45$^{\\circ}$N', '42$^{\\circ}$N']  # Specify corresponding labels\n",
    "# xtick_positions = [0, 12, 17]  # Specify the positions where you want ticks\n",
    "# xtick_labels = ['0%', '71%', '100%']  # Specify corresponding labels\n",
    "# Set the custom x-axis ticks\n",
    "plt.xticks(xtick_positions, xtick_labels)\n",
    "\n",
    "plt.legend(fontsize=14, loc='upper center')\n",
    "\n",
    "# plt.savefig(f'iRF_prediction_eurocontrol.png', dpi=300, bbox_inches='tight') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc8b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
